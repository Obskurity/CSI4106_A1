{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSI 4106 Introduction to Artificial Intelligence** <br/>\n",
    "*Assignment 1: Data Preparation*\n",
    "\n",
    "# Identification\n",
    "\n",
    "Name: Arthur Wang <br/>\n",
    "Student Number: 300185298\n",
    "\n",
    "# Exploratory Analysis\n",
    "\n",
    "## Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "\n",
    "As outlined in the project description, it should be possible for the correctors to ecute your notebook without requiring any downloads.\n",
    "\n",
    "To facilitate access to the dataset without the need for downloads, use the data ovided in the public GitHub repository and provide a link to the raw version of the taset.\n",
    "\n",
    "The link to the raw version is as follows:\n",
    "\n",
    "*https://raw.githubusercontent.com/GITHUB_USERNAME/REPOSITORY_NAME/main/DATASETNAME.v*\n",
    "\n",
    "For example:\n",
    "\n",
    "[https://github.com/turcotte/csi4106-f24/blob/main/assignments-data/a1/01/glass.csv]ttps://github.com/turcotte/csi4106-f24/blob/main/assignments-data/a1/01/glass.csv)\n",
    "\n",
    "Now provide the link to YOUR dataset and read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading glass identification dataset\n",
    "glass_id_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/01/glass.csv\"\n",
    "glass_id_dataset = pd.read_csv(glass_id_url)\n",
    "\n",
    "# reading dermatology dataset\n",
    "dermatology_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/02/dermatology_database_1.csv\"\n",
    "dermatology_dataset = pd.read_csv(dermatology_url)\n",
    "\n",
    "# reading maternal health risk dataset\n",
    "mhr_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/03/Maternal%20Health%20Risk%20Data%20Set.csv\"\n",
    "mhr_dataset = pd.read_csv(mhr_url)\n",
    "\n",
    "# reading car dataset\n",
    "car_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/04/car.data\"\n",
    "car_dataset = pd.read_csv(car_url)\n",
    "\n",
    "# reading wine quality dataset\n",
    "wq_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/05/WineQT.csv\"\n",
    "wq_dataset = pd.read_csv(wq_url)\n",
    "\n",
    "# reading 16 personalities dataset\n",
    "# need to specify encoding or I run into errors\n",
    "sixteen_p_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/06/16P.csv\"\n",
    "sixteen_p_dataset = pd.read_csv(sixteen_p_url, encoding='ISO-8859-1')\n",
    "\n",
    "# reading credit score dataset\n",
    "# using training set only since it has sufficient data (100001 rows)\n",
    "cs_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/07/train.csv\"\n",
    "cs_dataset = pd.read_csv(cs_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    "The following are the questions for Assignment 1. Under each question, we have provided an initial code cell. You are encouraged to add additional code cells to maintain logical separation of your code. For instance, place the definition of a function in one cell and its execution in a subsequent cell. This approach will help preserve clarity and enhance readability by avoiding the inclusion of excessive code within a single cell.\n",
    "\n",
    "1. **Analysis of Missing Values**: Examine the datasets to identify and assess ssing values in various attributes. Missing values may be represented by symbols ch as '?', empty strings, or other placeholders.\n",
    "\n",
    "    1.1 In the list of options, what are the datasets that contain missing values? ecifically, which attribute or attributes has missing values?\n",
    "\n",
    "    1.2 Describe the methodology used for this investigation, and provide the rresponding code.\n",
    "\n",
    "    1.1 Data imputation involves replacing missing or incomplete data with substituted values to preserve the dataset's integrity for subsequent analysis. Propose imputation strategies for each attribute with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1**\n",
    "\n",
    "The datasets that contain missing values are the dermatology dataset, and the credit score dataset.\n",
    "\n",
    "In the dermatology dataset, missing values are denoted by a '?,' and attribute with missing values is 'age.'\n",
    "\n",
    "In the credit score dataset, missing values are denoted by either a null, '\\_,' or '_______' value. The\n",
    "attributes with missing values in the credit score dataset are: 'Name', 'Occupation', 'Monthly_Inhand_Salary', 'Type_of_Loan', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Credit_Mix', 'Credit_History_Age', 'Amount_invested_monthly', 'Monthly_Balance.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** \n",
    "\n",
    "The methodology used for this investigation is as follows: \n",
    "\n",
    "Prior to reading the information on each dataset, I performed null value checks for all features in the dataset.\n",
    "\n",
    "Based on the original datasets sources and their descriptions, the glass identification dataset has no missing values, the dermatology’s dataset’s missing values are denoted by a ‘?,’ the maternal health risk dataset has no missing values, and the car dataset has no missing values, and the 'Name' attribute in the credit score training dataset contained null values. The rest of the datasets and their respective attributes had no indication on their original dataset’s source. This information helped me verify that some of my null value checks were indeed correct.\n",
    "\n",
    "Finally, I went through each dataset individually to check if missing values were denoted with a special character(given that it was not mentioned in the datasets’ source’s description). For instance, occupation in credit_score was denoted ‘_______.’ And for other attributes in the credit score dataset, missing values were set with ‘_.’ I assumed these special characters were missing values because the attributes were categorical values.\n",
    "\n",
    "To check for empty missing values using pandas, we first count the sum of null/'?'/'_'/'_____' values in each column, then we sum those missing values per column to get the total missing values. If the resulting sum is greater than 0 then missing values exist.  To figure out which columns have the missing values we index and look for which columns are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "# 1.2a - Finding out which datasets have missing values\n",
    "\n",
    "# missing attribute values: None (dataset origin says so)\n",
    "# performed the following check prior to knowing this\n",
    "glass_missing = glass_id_dataset.isnull().sum().sum()\n",
    "print(\"Missing null values in the glass identification dataset:\", glass_missing)\n",
    "\n",
    "# performed the following check prior to knowing this\n",
    "dermatology_missing_null = dermatology_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the dermatology dataset:\", dermatology_missing_null)\n",
    "# missing values represented with '?' (dataset origin says so)\n",
    "dermatology_missing = (dermatology_dataset == '?').sum().sum()\n",
    "print(\"Missing ? values in the dermatology dataset:\", dermatology_missing)\n",
    "\n",
    "# has no missing values (dataset origin says so)\n",
    "# performed the following check prior to knowing this\n",
    "mhr_missing = mhr_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the maternal health risk dataset:\", mhr_missing)\n",
    "\n",
    "# has no missing values (dataset origin says so)\n",
    "# performed the following check prior to knowing this\n",
    "car_missing = car_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the car dataset:\", car_missing)\n",
    "\n",
    "# wine quality dataset\n",
    "# performed the following check prior to knowing this\n",
    "wq_missing = wq_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the wine quality dataset:\", wq_missing)\n",
    "\n",
    "# 16 personalities dataset\n",
    "sixteen_p_missing = sixteen_p_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the 16 personalities dataset:\", sixteen_p_missing)\n",
    "\n",
    "# credit score dataset | based on observation, I have found in this that missing values are represented as '______', '_', or null.\n",
    "cs_missing = cs_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the credit score dataset:\", cs_missing)\n",
    "l_underscore_count = (cs_dataset == '_______').sum().sum()\n",
    "print(\"Missing _______ values in the credit score dataset:\", l_underscore_count)\n",
    "cs_missing  += l_underscore_count\n",
    "underscore_count = (cs_dataset == '_').sum().sum()\n",
    "print(\"Missing _ values in the credit score dataset:\", underscore_count)\n",
    "cs_missing  += underscore_count\n",
    "print(\"Total missing values in the credit score dataset:\", cs_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2b - finding out which attributes have missing value in datastees that have missing values\n",
    "# As we can observe from the previous results, the dermatology and credit score have missing values\n",
    "# we know that the credit score dataset only has missing values that are '?'\n",
    "missing_attributes_dermatology = dermatology_dataset.columns[(dermatology_dataset == '?').any()]\n",
    "print(\"Attribute(s) with missing values for dermatology dataset: \", missing_attributes_dermatology)\n",
    "\n",
    "# based on observation, I have found in this that missing values are represented as '______', '_', or null.\n",
    "missing_attributes_cs = cs_dataset.columns[cs_dataset.isnull().any()]\n",
    "print(\"\\nAttribute(s) with missing values using null for credit score dataset: \", missing_attributes_cs)\n",
    "lu_missing_attributes_cs = cs_dataset.columns[(cs_dataset == '_______').any()]\n",
    "print(\"\\nAttribute(s) with missing values using _______ for credit score dataset: \", lu_missing_attributes_cs)\n",
    "u_missing_attributes_cs = cs_dataset.columns[(cs_dataset == '_').any()]\n",
    "print(\"\\nAttribute(s) with missing values using _ for credit score dataset: \", u_missing_attributes_cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Data Imputation Strategies**\n",
    "\n",
    "<u>Dermatology Dataset</u>\n",
    "\n",
    "For the ‘age’ attribute in the dermatology dataset, there are a variety of data imputation strategies we can use:\n",
    "The best data imputation strategy would be to K-Nearest-Neighbours regression(Turcotte, 2024). I propose using K-NN regression since I believe age is correlated with the other attributes given. Moreover, with K-NN, since age is the only missing attribute is age, we can use other attributes in our K-NN calculation. We use K-NN regression because age is a continous value. Finally, since the dataset is not huge(367 rows), thus computation will not be expensive(IBM, 2024).\n",
    "\n",
    "(reference slides with regression K-NN)\n",
    "\n",
    "\n",
    "<u>Credit Score Dataset</u>\n",
    "\n",
    "For the credit score dataset, a lot of the attributes have missing values:  'Name', 'Occupation', 'Monthly_Inhand_Salary', 'Type_of_Loan', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Credit_Mix', 'Credit_History_Age', 'Amount_invested_monthly', 'Monthly_Balance.'\n",
    "\n",
    "For the ‘Name’ attribute, the data imputation strategy we should use would be leaving the name as null, or setting it to a special value like ‘Unknown,’ or 'Not Specified.' This is because names are normally unique and should not be correlated with anything, thus we should not be trying to predicts its values if they are missing.\n",
    "\n",
    "For the categorical attributes like: ‘Occupation,’ ‘Type of Loan,’ and ‘Credit Mix’ attributes, I believe we can leverage K-NN but with classification instead of regression, since their possible values are discrete in nature. As a result, we can use other features in the dataset to help fill in these missing values. Another strategy we can use here is use some type of predictive modeling like a decision tree or random forest, however these require more work to set up (Jha 2022). Random forests are more accurate than decision trees and they do not overfit as much as decision trees, providing a more better way to predict these values. Thus, random forest can be used as an alternative to K-NN (Liberman 2020).\n",
    "\n",
    "For numerical attributes like ‘Monthly_Inhand_Salary,’ ‘Monthly_Balance,’ and ‘Amount_invested_monthly,’ we can assume these values are highly correlated with each other, and correlated with the ‘age,’ and ‘occupation’ attributes, most probably in a linear relationship. Thus, we can use linear regression as our data imputation strategy to fill in these missing values, since they are continuous and have a linear relationship with other attributes(that were listed) (Turcotte, 2024).\n",
    "\n",
    "For the remainder of the numerical attributes 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries','Credit_History_Age,' the data is continous and won't always have a linear relationship with other features, thus K-NN regression will work best for these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Select and familiarize yourself with a classification task:** Choose one of e provided datasets for further investigation. It is advisable to select a dataset ntaining a sufficiently large number of examples, ideally around 1,000, to ensure bust results when applying machine learning algorithms in the subsequent assignment.\n",
    "\n",
    "    2.1 What is the objective of the task? Is it intended for a specific plication? Do you possess expertise in this particular domain of application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** \n",
    "I am choosing the wine quality dataset. There are 1144 features in this dataset so it should be sufficient.\n",
    "\n",
    "**2.1** \n",
    "The objective of this task is to anticipate/predict the quality of wine given chemical attributes/characteristics that help determine if a wine is good quality or not. The application of this dataset is the quality of wine. I do not have expertise in the understanding the quality of wine based on the given chemical attributes, and thus the reasoning behind using the dataset is because it has sufficient data, has no missing values, and will help me to better understand the relationship between chemical characteristics of the wine and how it influences the wine\n",
    "quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Attribute Analysis**: \n",
    "\n",
    "    3.1 Determine which attributes lack informativeness and should be excluded to prove the effectiveness of the machine learning analysis. If all features are emed relevant, explicitly state this conclusion.\n",
    "\n",
    "    3.2 Examine the distribution of each attribute (column) within the dataset. Utilize histograms or boxplots to visualize the distributions, identifying any underlying patterns or outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1**\n",
    "\n",
    "The attributes that lack informativeness are only the ID attribute, since that is just to uniquely identify each wine, however will not be useful for machine learning analysis. The remainder of the attributes are relevant to determining the quality of wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq_dataset = wq_dataset.drop(columns=['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** \n",
    "\n",
    "Code Below. \n",
    "In the code, I am using a histogram to to visualize the distributions and identify patterns.\n",
    "To calculate number of bins of the histograms, I used the Freedman-Diaconis rule which is: *h = 2 x IQR x n^(-1/3)* (Hyndman, 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating bin numbers for histogram using the Freedman-Diaconis rule\n",
    "def num_bins(column_name, df):\n",
    "    c = df[column_name]\n",
    "    Q1 = np.percentile(c, 25)  \n",
    "    Q3 = np.percentile(c, 75)  \n",
    "    IQR = Q3 - Q1\n",
    "    bin_width = (2 * IQR) / (len(c) ** (1/3))\n",
    "    num_bins = int((c.max() - c.min()) / bin_width)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('fixed acidity', wq_dataset)\n",
    "\n",
    "# Fixed Acidity Attribute\n",
    "wq_dataset['fixed acidity'].hist(bins=n)\n",
    "plt.title('Histogram of Fixed Acidity')\n",
    "plt.xlabel('Fixed Acidity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Fixed Acidity' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers > 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('volatile acidity', wq_dataset)\n",
    "\n",
    "# Volatile Acidity Attribute\n",
    "wq_dataset['volatile acidity'].hist(bins=n)\n",
    "plt.title('Histogram of Volatile Acidity')\n",
    "plt.xlabel('Volatile Acidity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Volatile Acidity' attribute, the data from this attribute is slightly right/positively skewed, and is not symmetric. There exists outliers > 1.1 value, on the right of the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('citric acid', wq_dataset)\n",
    "\n",
    "# Citric Acid Attribute\n",
    "wq_dataset['citric acid'].hist(bins=n)\n",
    "plt.title('Histogram of Citric Acid')\n",
    "plt.xlabel('Citric Acid')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Citric Acid' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers, which values > 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('residual sugar', wq_dataset)\n",
    "\n",
    "# Residual Sugar Attribute\n",
    "wq_dataset['residual sugar'].hist(bins=n)\n",
    "plt.title('Histogram of Residual Sugar')\n",
    "plt.xlabel('Residual Sugar')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Residual Sugar' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers > value 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('free sulfur dioxide', wq_dataset)\n",
    "\n",
    "# Free Sulfur Dioxide Attribute\n",
    "wq_dataset['free sulfur dioxide'].hist(bins=n)\n",
    "plt.title('Histogram of Free Sulfur Dioxide')\n",
    "plt.xlabel('Free Sulfur Dioxide')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Free Sulfur Dioxide' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers for values > 65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('total sulfur dioxide', wq_dataset)\n",
    "\n",
    "# Total Sulfur Dioxide Attribute\n",
    "wq_dataset['total sulfur dioxide'].hist(bins=n)\n",
    "plt.title('Histogram of Total Sulfur Dioxide')\n",
    "plt.xlabel('Total Sulfur Dioxide')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Total Sulfur Dioxide' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists an outlier value > 260."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('density', wq_dataset)\n",
    "\n",
    "# Density Attribute\n",
    "wq_dataset['density'].hist(bins=n)\n",
    "plt.title('Histogram of Density')\n",
    "plt.xlabel('Density')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Density' attribute, the density attribute's data has close to a normal distirbution and is roughly symmetric. There are no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('pH', wq_dataset)\n",
    "\n",
    "# pH Attribute\n",
    "wq_dataset['pH'].hist(bins=n)\n",
    "plt.title('Histogram of pH')\n",
    "plt.xlabel('pH')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'pH' attribute, the pH attribute's data is close to a normal distribution, and is roughly symmetric. There exists outiers values < 2.8, and 2 outliers values > 3.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('sulphates', wq_dataset)\n",
    "\n",
    "# Sulphates Attribute\n",
    "wq_dataset['sulphates'].hist(bins=n)\n",
    "plt.title('Histogram of Sulphates')\n",
    "plt.xlabel('Sulphate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Sulphates' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers on values > 1.26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('alcohol', wq_dataset)\n",
    "\n",
    "# Alcohol Attribute\n",
    "wq_dataset['alcohol'].hist(bins=n)\n",
    "plt.title('Histogram of Alcohol')\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Alcohol' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers with values >= 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 General Distribution and Outlier Analysis**\n",
    "\n",
    "After looking at all the attributes and their distributions, the majority of graphs are right/positively skewed. The majority of these right/positively skewed graphs have outliers that exist onthe right side of the graph.\n",
    "\n",
    "There are 2 graphs with close to a normal distribution, one with no outliers, and the other has outliers on both the left and right side of extreme/outlier values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Class Distribution Analysis**: Investigate the distribution of class labels within the dataset. Employ bar plots to visualize the frequency of instances for each class, and assess whether the dataset is balanced or imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "sorted = wq_dataset['quality'].value_counts()\n",
    "sorted.plot(kind='bar')\n",
    "plt.title('Frequency of Quality Class Labels')\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# used code from: https://www.w3schools.com/python/matplotlib_plotting.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class labels for this dataset is the quality, and as we can observe, the datset is imbalanced, as most of the values of the quality class label are 5 or 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in class we learned that for numerical processing, we should split the dataset then preprocess so that there isn't information leakage from processing numerical attributes. Thus, we will be doing **question 6**, then **question 7**, then **question 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Training and target data**: Set the Python variable `X` to designate the data and `y` to designate the target class. Make sure to select only the informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "# the only non-informative feature was already dropped, so all existing features and not the quality feature are informative\n",
    "X = wq_dataset.drop(columns=['quality'])\n",
    "\n",
    "# quality is the target class\n",
    "y = wq_dataset['quality']\n",
    "\n",
    "print(X)    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Training and test sets**: Split the dataset into training and testing sets. Reserve 20% of data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# (Turcotte, 2024)\n",
    "# code taken from https://turcotte.xyz/teaching/csi-4106/lectures/05/slides.html#/one-vs-all-classifier-complete\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Preprocessing**: \n",
    "\n",
    "    5.1 For numerical features, determine the best transformation to use. Indicate e transformation that seems appropriate and why. Include the code illustrating how  apply the transformation. For at least one attribute, show the distribution before d after the transformation. See [Preprocessing data](https://scikit-learn.org/able/modules/preprocessing.html).\n",
    "\n",
    "    5.2 For categorical features, show how to apply [one-hot encoding](https://ikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).  your dataset does not have categorical data, show how to apply the one-hot encoder  the label (target variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1a**\n",
    "\n",
    "<u>Preprocessing for features with a normal distribution</u>\n",
    "\n",
    "As we can observe from the histograms in sections 3, we know that only two features have close to a normal distribution: 'density,' and 'pH.' The 'density' feature has a normal distribution without outliers, and the 'pH' feature has a normal distribution with outliers. Thus, since they both have a normal distribution, we want to use standardization, however with the 'pH' feature, since it contains outliers, we want to use robust standardization/the robust scaler since 'scaling using the mean and variance of the data is likely to not work very well [for features with outliers]' (scikit, n.d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store previous state of data before preprocessing\n",
    "X_train_before = X_train\n",
    "X_test_before = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before preprocessing/standardization - training set\n",
    "# density feature\n",
    "# pH feature\n",
    "n = num_bins('pH', X_train_before)\n",
    "X_train_before['pH'].hist(bins=n)\n",
    "plt.title('Histogram of pH before Standardizing')\n",
    "plt.xlabel('pH')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set Preprocessing\n",
    "# density feature - normal distribution without outliers\n",
    "scaler = preprocessing.StandardScaler().fit(X_train[['density']])\n",
    "density_standardized = scaler.transform(X_train[['density']])\n",
    "X_train['density'] = density_standardized\n",
    "# code used from https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "# pH feature - normal distribution with outliers, using robust scaler standardization\n",
    "transformer = RobustScaler().fit(X_train[['pH']])\n",
    "transformer\n",
    "pH_standardized = transformer.transform(X_train[['pH']])\n",
    "X_train['pH'] = pH_standardized\n",
    "# code used from https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After preprocessing/standardization - training set\n",
    "# pH feature\n",
    "n = num_bins('pH', X_train)\n",
    "X_train['pH'].hist(bins=n)\n",
    "plt.title('Histogram of pH after Standardizing')\n",
    "plt.xlabel('pH')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Preprocessing\n",
    "# density feature - normal distribution without outliers\n",
    "scaler = preprocessing.StandardScaler().fit(X_test[['density']])\n",
    "density_standardized = scaler.transform(X_test[['density']])\n",
    "X_test['density'] = density_standardized\n",
    "# code used from https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "# pH feature - normal distribution with outliers, using robust scaler standardization\n",
    "transformer = RobustScaler().fit(X_test[['pH']])\n",
    "transformer\n",
    "pH_standardized = transformer.transform(X_test[['pH']])\n",
    "X_test['pH'] = pH_standardized\n",
    "# code used from https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1b**\n",
    "\n",
    "<u>Preprocessing for features with a right skewed distribution</u>\n",
    "\n",
    "As we can observe from the histograms in section 3, we know that the remainder of the features do not have a normal distribution, but are skewed to the right/are positively skewed. All attributes except 'citric acid' is > 0, thus we will use the Box-Cox transformation because it is effective and the best way to transform right skewed data, and use yeo-johnson for citric acid, since they are both effective methods for right skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set before:\n",
    "# Before transforming fixed acidity\n",
    "n = num_bins('fixed acidity', X_train_before)\n",
    "# Fixed Acidity Attribute\n",
    "X_train_before['fixed acidity'].hist(bins=n)\n",
    "plt.title('Histogram of Fixed Acidity Before Transformation')\n",
    "plt.xlabel('Fixed Acidity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Before transforming total sulfur dioxide acid\n",
    "n = num_bins('total sulfur dioxide', X_train_before)\n",
    "# Fixed Acidity Attribute\n",
    "X_train_before['total sulfur dioxide'].hist(bins=n)\n",
    "plt.title('Histogram of Citric Acid Before Transformation')\n",
    "plt.xlabel('Citric Acid')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Preprocessing Training Set</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bc = PowerTransformer(method='box-cox', standardize=False)\n",
    "res = pt_bc.fit_transform(X_train[['fixed acidity', 'volatile acidity', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'alcohol']])\n",
    "X_train[['fixed acidity', 'volatile acidity', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'alcohol']] = res\n",
    "\n",
    "pt_yj = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "res_yj = pt_yj.fit_transform(X_train[['citric acid']])\n",
    "X_train['citric acid'] = res_yj\n",
    "# code used from https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set after:\n",
    "# Before normalizing fixed acidity\n",
    "n = num_bins('fixed acidity', X_train)\n",
    "# Fixed Acidity Attribute\n",
    "X_train['fixed acidity'].hist(bins=n)\n",
    "plt.title('Histogram of Fixed Acidity After Transformation')\n",
    "plt.xlabel('Fixed Acidity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After transforming total sulfur dioxide\n",
    "n = num_bins('total sulfur dioxide', X_train)\n",
    "# Fixed Acidity Attribute\n",
    "X_train['total sulfur dioxide'].hist(bins=n)\n",
    "plt.title('Histogram of Citric Acid After Transformation')\n",
    "plt.xlabel('Citric Acid')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Preprocessing test set</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bc_test = PowerTransformer(method='box-cox', standardize=False)\n",
    "res_test = pt_bc.fit_transform(X_test[['fixed acidity', 'volatile acidity', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'alcohol']])\n",
    "X_test[['fixed acidity', 'volatile acidity', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'alcohol']] = res_test\n",
    "\n",
    "pt_yj_test = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "res_yj_test = pt_yj_test.fit_transform(X_test[['citric acid']])\n",
    "X_test['citric acid'] = res_yj_test\n",
    "# code used from https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer\n",
    "\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 - Applying one-hot encoder to the label/target variable (quality).\n",
    "# My dataset does not have categorical data, so I will show how to apply the one-hot encoder to the quality attribute (target variable)\n",
    "\n",
    "# using copy of dataset since I am showing how to apply the one-hot encoder, but it's not actually needed\n",
    "wq_dataset_copy = wq_dataset\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "res = enc.fit_transform(wq_dataset_copy[['quality']])\n",
    "# (sckit)\n",
    "# used code from: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "\n",
    "# adding one-hot encoded target attribute back to the dataset\n",
    "encoded_cols = pd.DataFrame(res, columns=enc.get_feature_names_out(['quality']))\n",
    "encoded_cols.index = wq_dataset_copy.index\n",
    "wq_dataset_copy = pd.concat([wq_dataset_copy, encoded_cols], axis=1)\n",
    "wq_dataset_copy = wq_dataset_copy.drop(columns=['quality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "\n",
    "# References\n",
    "\n",
    "Hyndman, R. (2010, July 28). *Calculating optimal number of bins in a histogram*. StackExchange. https://stats.stackexchange.com/a/862\n",
    "\n",
    "IBM. (2024, August 14). *What is the K-nearest neighbors algorithm?*. IBM. https://www.ibm.com/topics/knn\n",
    "\n",
    "Jha, S. (2022). *Handling Missing Values with Random Forest*. Kaggle. https://www.kaggle.com/discussions/general/336711\n",
    "\n",
    "Liberman, N. (2020, May 21). *Decision trees and random forests*. Medium. https://towardsdatascience.com/decision-trees-and-random-forests-df0c3123f991\n",
    "\n",
    "*Onehotencoder*. scikit. (n.d.). https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html \n",
    "\n",
    "*Preprocessing.* scikit. (n.d.). https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "*RobustScaler.* scikit. (n.d.).https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.RobustScaler.htmlz\n",
    "\n",
    "Turcotte, M. (2024, September 23). *Linear models, logistic regression*. https://turcotte.xyz/teaching/csi-4106/lectures/05/slides.html#/one-vs-all-classifier-complete\n",
    "\n",
    "Turcotte, M. (2024, September 25). *Linear models, training*. https://turcotte.xyz/teaching/csi-4106/lectures/04/slides.html#/\n",
    "\n",
    "W3Schools. (n.d.). *Matplotlib Plotting*. https://www.w3schools.com/python/matplotlib_plotting.asp \n",
    "\n",
    "## AI transcript\n",
    "**Hint:** To share a link to your colab notebook, click on \"share\" on the top right. Then, under *General access* , change *Restricted* to \"Anyone with the link\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what type of preprocessing for scikit learn for right skewed data\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "When dealing with right-skewed data in scikit-learn, several preprocessing techniques can help transform the data for better modeling. Here’s a breakdown of common preprocessing steps:\n",
    "\n",
    "1. Log Transformation\n",
    "Log transformation is one of the most effective ways to reduce right skewness. It compresses the range of the data and can make the distribution more normal.\n",
    "\n",
    "How to do it:\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "\n",
    "wq_dataset['fixed acidity'] = np.log1p(wq_dataset['fixed acidity'])  # Use log(1+x) to handle zeros\n",
    "2. Square Root Transformation\n",
    "Similar to log transformation, square root transformation can also help reduce skewness.\n",
    "\n",
    "How to do it:\n",
    "python\n",
    "Copy code\n",
    "wq_dataset['fixed acidity'] = np.sqrt(wq_dataset['fixed acidity'])\n",
    "3. Box-Cox Transformation\n",
    "Box-Cox transformation is a more generalized method that can be applied to positive data to stabilize variance and make the data more normally distributed. It requires all values to be positive.\n",
    "\n",
    "How to do it:\n",
    "python\n",
    "Copy code\n",
    "from scipy import stats\n",
    "\n",
    "wq_dataset['fixed acidity'], _ = stats.boxcox(wq_dataset['fixed acidity'])\n",
    "4. Yeo-Johnson Transformation\n",
    "Yeo-Johnson transformation is similar to Box-Cox but can handle zero and negative values.\n",
    "\n",
    "How to do it:\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "wq_dataset['fixed acidity'] = pt.fit_transform(wq_dataset[['fixed acidity']])\n",
    "5. Standardization (Z-score Normalization)\n",
    "After reducing skewness, you may want to standardize your data to have a mean of 0 and a standard deviation of 1. This is particularly useful for algorithms that assume normally distributed data.\n",
    "\n",
    "How to do it:\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "wq_dataset['fixed acidity'] = scaler.fit_transform(wq_dataset[['fixed acidity']])\n",
    "6. Min-Max Scaling\n",
    "If you want to scale your data to a specific range (e.g., 0 to 1), you can apply Min-Max scaling.\n",
    "\n",
    "How to do it:\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "wq_dataset['fixed acidity'] = scaler.fit_transform(wq_dataset[['fixed acidity']])\n",
    "Summary of Steps\n",
    "Transform the Data:\n",
    "\n",
    "Apply log, square root, Box-Cox, or Yeo-Johnson transformation to reduce skewness.\n",
    "Scale the Data:\n",
    "\n",
    "Standardize or Min-Max scale the data after transformation if needed.\n",
    "Example Workflow\n",
    "Here’s how you can put it all together:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "\n",
    "# Sample DataFrame\n",
    "wq_dataset = pd.DataFrame({\n",
    "    'fixed acidity': [7.4, 7.8, 11.2, 7.4, 7.9, 6.7, 5.6, 7.8, 8.5, 7.9]\n",
    "})\n",
    "\n",
    "# Step 1: Apply Yeo-Johnson transformation to reduce skewness\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "wq_dataset['fixed acidity'] = pt.fit_transform(wq_dataset[['fixed acidity']])\n",
    "\n",
    "# Step 2: Standardize the transformed data\n",
    "scaler = StandardScaler()\n",
    "wq_dataset['fixed acidity'] = scaler.fit_transform(wq_dataset[['fixed acidity']])\n",
    "\n",
    "# Check the transformed data\n",
    "print(wq_dataset)\n",
    "Conclusion\n",
    "Choosing the right preprocessing technique for right-skewed data is essential for improving model performance. Transformations like log or Yeo-Johnson can effectively address skewness, while scaling methods like standardization or min-max scaling prepare the data for many machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
