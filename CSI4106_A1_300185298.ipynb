{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSI 4106 Introduction to Artificial Intelligence** <br/>\n",
    "*Assignment 1: Data Preparation*\n",
    "\n",
    "# Identification\n",
    "\n",
    "Name: Arthur Wang <br/>\n",
    "Student Number: 300185298\n",
    "\n",
    "# Exploratory Analysis\n",
    "\n",
    "## Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "\n",
    "As outlined in the project description, it should be possible for the correctors to ecute your notebook without requiring any downloads.\n",
    "\n",
    "To facilitate access to the dataset without the need for downloads, use the data ovided in the public GitHub repository and provide a link to the raw version of the taset.\n",
    "\n",
    "The link to the raw version is as follows:\n",
    "\n",
    "*https://raw.githubusercontent.com/GITHUB_USERNAME/REPOSITORY_NAME/main/DATASETNAME.v*\n",
    "\n",
    "For example:\n",
    "\n",
    "[https://github.com/turcotte/csi4106-f24/blob/main/assignments-data/a1/01/glass.csv]ttps://github.com/turcotte/csi4106-f24/blob/main/assignments-data/a1/01/glass.csv)\n",
    "\n",
    "Now provide the link to YOUR dataset and read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading glass identification dataset\n",
    "glass_id_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/01/glass.csv\"\n",
    "glass_id_dataset = pd.read_csv(glass_id_url)\n",
    "\n",
    "# reading dermatology dataset\n",
    "dermatology_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/02/dermatology_database_1.csv\"\n",
    "dermatology_dataset = pd.read_csv(dermatology_url)\n",
    "\n",
    "# reading maternal health risk dataset\n",
    "mhr_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/03/Maternal%20Health%20Risk%20Data%20Set.csv\"\n",
    "mhr_dataset = pd.read_csv(mhr_url)\n",
    "\n",
    "# reading car dataset\n",
    "car_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/04/car.data\"\n",
    "car_dataset = pd.read_csv(car_url)\n",
    "\n",
    "# reading wine quality dataset\n",
    "wq_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/05/WineQT.csv\"\n",
    "wq_dataset = pd.read_csv(wq_url)\n",
    "\n",
    "# reading 16 personalities dataset\n",
    "sixteen_p_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/06/16P.csv\"\n",
    "sixteen_p_dataset = pd.read_csv(sixteen_p_url, encoding='ISO-8859-1')\n",
    "\n",
    "# reading credit score dataset\n",
    "# using training set only since it has sufficient data (100001 rows)\n",
    "cs_url = \"https://raw.githubusercontent.com/Obskurity/CSI4106_A1/main/07/train.csv\"\n",
    "cs_dataset = pd.read_csv(cs_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    "The following are the questions for Assignment 1. Under each question, we have provided an initial code cell. You are encouraged to add additional code cells to maintain logical separation of your code. For instance, place the definition of a function in one cell and its execution in a subsequent cell. This approach will help preserve clarity and enhance readability by avoiding the inclusion of excessive code within a single cell.\n",
    "\n",
    "1. **Analysis of Missing Values**: Examine the datasets to identify and assess ssing values in various attributes. Missing values may be represented by symbols ch as '?', empty strings, or other placeholders.\n",
    "\n",
    "    1.1 In the list of options, what are the datasets that contain missing values? ecifically, which attribute or attributes has missing values?\n",
    "\n",
    "    1.2 Describe the methodology used for this investigation, and provide the rresponding code.\n",
    "\n",
    "    1.1 Data imputation involves replacing missing or incomplete data with substituted values to preserve the dataset's integrity for subsequent analysis. Propose imputation strategies for each attribute with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1**\n",
    "\n",
    "The datasets that contain missing values are the dermatology dataset, and the credit score dataset.\n",
    "\n",
    "In the dermatology dataset, missing values are denoted by a '?,' and attribute with missing values is 'age.'\n",
    "\n",
    "In the credit score dataset, missing values are denoted by either a null, '\\_,' or '_______' value. The\n",
    "attributes with missing values in the credit score dataset are: 'Name', 'Occupation', 'Monthly_Inhand_Salary', 'Type_of_Loan', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Credit_Mix', 'Credit_History_Age', 'Amount_invested_monthly', 'Monthly_Balance.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** \n",
    "\n",
    "The methodology used for this investigation is as follows: \n",
    "\n",
    "Prior to reading the information on each dataset, I performed null value checks for all features in the dataset.\n",
    "\n",
    "Based on the original datasets sources and their descriptions, the glass identification dataset has no missing values, the dermatology’s dataset’s missing values are denoted by a ‘?,’ the maternal health risk dataset has no missing values, and the car dataset has no missing values, and the 'Name' attribute in the credit score training dataset contained null values. The rest of the datasets and their respective attributes had no indication on their original dataset’s source. This information helped me verify that some of my null value checks were indeed correct.\n",
    "\n",
    "Finally, I went through each dataset individually to check if missing values were denoted with a special character(given that it was not mentioned in the datasets’ source’s description). For instance, occupation in credit_score was denoted ‘_______.’ And for other attributes in the credit score dataset, missing values were set with ‘_.’ I assumed these special characters were missing values because the attributes were categorical values.\n",
    "\n",
    "To check for empty missing values using pandas, we first count the sum of null/'?'/'_'/'_____' values in each column, then we sum those missing values per column to get the total missing values. If the resulting sum is greater than 0 then missing values exist.  To figure out which columns have the missing values we index and look for which columns are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "# 1.2a - Finding out which datasets have missing values\n",
    "\n",
    "# missing attribute values: None (dataset origin says so)\n",
    "# performed the following check prior to knowing this\n",
    "glass_missing = glass_id_dataset.isnull().sum().sum()\n",
    "print(\"Missing null values in the glass identification dataset:\", glass_missing)\n",
    "\n",
    "# performed the following check prior to knowing this\n",
    "dermatology_missing_null = dermatology_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the dermatology dataset:\", dermatology_missing_null)\n",
    "# missing values represented with '?' (dataset origin says so)\n",
    "dermatology_missing = (dermatology_dataset == '?').sum().sum()\n",
    "print(\"Missing ? values in the dermatology dataset:\", dermatology_missing)\n",
    "\n",
    "# has no missing values (dataset origin says so)\n",
    "# performed the following check prior to knowing this\n",
    "mhr_missing = mhr_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the maternal health risk dataset:\", mhr_missing)\n",
    "\n",
    "# has no missing values (dataset origin says so)\n",
    "# performed the following check prior to knowing this\n",
    "car_missing = car_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the car dataset:\", car_missing)\n",
    "\n",
    "# wine quality dataset\n",
    "# performed the following check prior to knowing this\n",
    "wq_missing = wq_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the wine quality dataset:\", wq_missing)\n",
    "\n",
    "# 16 personalities dataset\n",
    "sixteen_p_missing = sixteen_p_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the 16 personalities dataset:\", sixteen_p_missing)\n",
    "\n",
    "# credit score dataset | based on observation, I have found in this that missing values are represented as '______', '_', or null.\n",
    "cs_missing = cs_dataset.isnull().sum().sum()\n",
    "print(\"\\nMissing null values in the credit score dataset:\", cs_missing)\n",
    "l_underscore_count = (cs_dataset == '_______').sum().sum()\n",
    "print(\"Missing _______ values in the credit score dataset:\", l_underscore_count)\n",
    "cs_missing  += l_underscore_count\n",
    "underscore_count = (cs_dataset == '_').sum().sum()\n",
    "print(\"Missing _ values in the credit score dataset:\", underscore_count)\n",
    "cs_missing  += underscore_count\n",
    "print(\"Total missing values in the credit score dataset:\", cs_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2b - finding out which attributes have missing value in datastees that have missing values\n",
    "# As we can observe from the previous results, the dermatology and credit score have missing values\n",
    "# we know that the credit score dataset only has missing values that are '?'\n",
    "missing_attributes_dermatology = dermatology_dataset.columns[(dermatology_dataset == '?').any()]\n",
    "print(\"Attribute(s) with missing values for dermatology dataset: \", missing_attributes_dermatology)\n",
    "\n",
    "# based on observation, I have found in this that missing values are represented as '______', '_', or null.\n",
    "missing_attributes_cs = cs_dataset.columns[cs_dataset.isnull().any()]\n",
    "print(\"\\nAttribute(s) with missing values using null for credit score dataset: \", missing_attributes_cs)\n",
    "lu_missing_attributes_cs = cs_dataset.columns[(cs_dataset == '_______').any()]\n",
    "print(\"\\nAttribute(s) with missing values using _______ for credit score dataset: \", lu_missing_attributes_cs)\n",
    "u_missing_attributes_cs = cs_dataset.columns[(cs_dataset == '_').any()]\n",
    "print(\"\\nAttribute(s) with missing values using _ for credit score dataset: \", u_missing_attributes_cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Data Imputation Strategies**\n",
    "\n",
    "<u>Dermatology Dataset</u>\n",
    "\n",
    "For the ‘age’ attribute in the dermatology dataset, there are a variety of data imputation strategies we can use:\n",
    "The best data imputation strategy would be to K-Nearest-Neighbours regression(Turcotte, 2024). I propose using K-NN regression since I believe age is correlated with the other attributes given. Moreover, with K-NN, since age is the only missing attribute is age, we can use other attributes in our K-NN calculation. We use K-NN regression because age is a continous value. Finally, since the dataset is not huge(367 rows), thus computation will not be expensive(IBM, 2024).\n",
    "\n",
    "(reference slides with regression K-NN)\n",
    "\n",
    "\n",
    "<u>Credit Score Dataset</u>\n",
    "\n",
    "For the credit score dataset, a lot of the attributes have missing values:  'Name', 'Occupation', 'Monthly_Inhand_Salary', 'Type_of_Loan', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Credit_Mix', 'Credit_History_Age', 'Amount_invested_monthly', 'Monthly_Balance.'\n",
    "\n",
    "For the ‘Name’ attribute, the data imputation strategy we should use would be leaving the name as null, or setting it to a special value like ‘Unknown,’ or 'Not Specified.' This is because names are normally unique and should not be correlated with anything, thus we should not be trying to predicts its values if they are missing.\n",
    "\n",
    "For the categorical attributes like: ‘Occupation,’ ‘Type of Loan,’ and ‘Credit Mix’ attributes, I believe we can leverage K-NN but with classification instead of regression, since their possible values are discrete in nature. As a result, we can use other features in the dataset to help fill in these missing values. Another strategy we can use here is use some type of predictive modeling like a decision tree or random forest, however these require more work to set up. Random forests are more accurate than decision trees and they do not overfit as much as decision trees, providing a more better way to predict these values. Thus, random forest can be used as an alternative to K-NN (Liberman 2020).\n",
    "\n",
    "For numerical attributes like ‘Monthly_Inhand_Salary,’ ‘Monthly_Balance,’ and ‘Amount_invested_monthly,’ we can assume these values are highly correlated with each other, and correlated with the ‘age,’ and ‘occupation’ attributes, most probably in a linear relationship. Thus, we can use linear regression as our data imputation strategy to fill in these missing values, since they are continuous and have a linear relationship with other attributes(that were listed). (slides)\n",
    "\n",
    "For the remainder of the numerical attributes 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries','Credit_History_Age,' the data is continous and won't always have a linear relationship with other features, thus K-NN regression will work best for these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Select and familiarize yourself with a classification task:** Choose one of e provided datasets for further investigation. It is advisable to select a dataset ntaining a sufficiently large number of examples, ideally around 1,000, to ensure bust results when applying machine learning algorithms in the subsequent assignment.\n",
    "\n",
    "    2.1 What is the objective of the task? Is it intended for a specific plication? Do you possess expertise in this particular domain of application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** \n",
    "I am choosing the wine quality dataset. There are 1144 features in this dataset so it should be sufficient.\n",
    "\n",
    "**2.1** \n",
    "The objective of this task is to anticipate/predict the quality of wine given chemical attributes/characteristics that help determine if a wine is good quality or not. The application of this dataset is the quality of wine. I do not have expertise in the understanding the quality of wine based on the given chemical attributes, and thus the reasoning behind using the dataset is because it has sufficient data, has no missing values, and will help me to better understand the relationship between chemical characteristics of the wine and how it influences the wine\n",
    "quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Attribute Analysis**: \n",
    "\n",
    "    3.1 Determine which attributes lack informativeness and should be excluded to prove the effectiveness of the machine learning analysis. If all features are emed relevant, explicitly state this conclusion.\n",
    "\n",
    "    3.2 Examine the distribution of each attribute (column) within the dataset. Utilize histograms or boxplots to visualize the distributions, identifying any underlying patterns or outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1**\n",
    "\n",
    "The attributes that lack informativeness are only the ID attribute, since that is just to uniquely identify each wine, however will not be useful for machine learning analysis. The remainder of the attributes are relevant to determining the quality of wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq_dataset = wq_dataset.drop(columns=['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** \n",
    "\n",
    "Code Below. \n",
    "In the code, I am using a histogram to to visualize the distributions and identify patterns.\n",
    "To calculate bin numbers of the histogram, I used the Freedman-Diaconis rule which is: *h = 2 x IQR x n^(-1/3)* (Hyndman, 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating bin numbers for histogram using the Freedman-Diaconis rule\n",
    "def num_bins(column_name):\n",
    "    c = wq_dataset[column_name]\n",
    "    Q1 = np.percentile(c, 25)  \n",
    "    Q3 = np.percentile(c, 75)  \n",
    "    IQR = Q3 - Q1\n",
    "    bin_width = (2 * IQR) / (len(c) ** (1/3))\n",
    "    num_bins = int((c.max() - c.min()) / bin_width)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('fixed acidity')\n",
    "\n",
    "# Fixed Acidity Attribute\n",
    "wq_dataset['fixed acidity'].hist(bins=n)\n",
    "plt.title('Histogram of Fixed Acidity')\n",
    "plt.xlabel('Fixed Acidity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Fixed Acidity' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers > 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('volatile acidity')\n",
    "\n",
    "# Volatile Acidity Attribute\n",
    "wq_dataset['volatile acidity'].hist(bins=n)\n",
    "plt.title('Histogram of Volatile Acidity')\n",
    "plt.xlabel('Volatile Acidity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Volatile Acidity' attribute, the data from this attribute is slightly right/positively skewed, and is not symmetric. It follows close to a poisson distribution.There exists outliers > 1.1 value, on the right of the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('citric acid')\n",
    "\n",
    "# Citric Acid Attribute\n",
    "wq_dataset['citric acid'].hist(bins=n)\n",
    "plt.title('Histogram of Citric Acid')\n",
    "plt.xlabel('Citric Acid')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Citric Acid' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers, which values > 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('residual sugar')\n",
    "\n",
    "# Residual Sugar Attribute\n",
    "wq_dataset['residual sugar'].hist(bins=n)\n",
    "plt.title('Histogram of Residual Sugar')\n",
    "plt.xlabel('Residual Sugar')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Residual Sugar' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers > value 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('free sulfur dioxide')\n",
    "\n",
    "# Free Sulfur Dioxide Attribute\n",
    "wq_dataset['free sulfur dioxide'].hist(bins=n)\n",
    "plt.title('Histogram of Free Sulfur Dioxide')\n",
    "plt.xlabel('Free Sulfur Dioxide')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Free Sulfur Dioxide' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers for values > 65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('total sulfur dioxide')\n",
    "\n",
    "# Total Sulfur Dioxide Attribute\n",
    "wq_dataset['total sulfur dioxide'].hist(bins=n)\n",
    "plt.title('Histogram of Total Sulfur Dioxide')\n",
    "plt.xlabel('Total Sulfur Dioxide')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Total Sulfur Dioxide' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists an outlier value > 260."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('density')\n",
    "\n",
    "# Density Attribute\n",
    "wq_dataset['density'].hist(bins=n)\n",
    "plt.title('Histogram of Density')\n",
    "plt.xlabel('Density')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Density' attribute, the density attribute's data has close to a normal distirbution and is roughly symmetric. There are no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('pH')\n",
    "\n",
    "# pH Attribute\n",
    "wq_dataset['pH'].hist(bins=n)\n",
    "plt.title('Histogram of pH')\n",
    "plt.xlabel('pH')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'pH' attribute, the pH attribute's data is close to a normal distribution, and is roughly symmetric. There exists outiers values < 2.8, and 2 outliers values > 3.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('sulphates')\n",
    "\n",
    "# Sulphates Attribute\n",
    "wq_dataset['sulphates'].hist(bins=n)\n",
    "plt.title('Histogram of Sulphates')\n",
    "plt.xlabel('Sulphate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Sulphates' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers on values > 1.26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_bins('alcohol')\n",
    "\n",
    "# Alcohol Attribute\n",
    "wq_dataset['alcohol'].hist(bins=n)\n",
    "plt.title('Histogram of Alcohol')\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe for the 'Alcohol' attribute, the data from this attribute is right/positively skewed, and is not symmetric. There exists outliers with values >= 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 General Distribution and Outlier Analysis**\n",
    "\n",
    "After looking at all the attributes and their distributions, the majority of graphs are right/positively skewed. The majority of these right/positively skewed graphs have outliers that exist onthe right side of the graph.\n",
    "\n",
    "There are 2 graphs with close to a normal distribution, one with no outliers, and the other has outliers on both the left and right side of extreme/outlier values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Class Distribution Analysis**: Investigate the distribution of class labels within the dataset. Employ bar plots to visualize the frequency of instances for each class, and assess whether the dataset is balanced or imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "sorted = wq_dataset['quality'].value_counts()\n",
    "sorted.plot(kind='bar')\n",
    "plt.title('Frequency of Quality Class Labels')\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# used code from: https://www.w3schools.com/python/matplotlib_plotting.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class labels for this dataset is the quality, and as we can observe, the datset is imbalanced, as most of the values of the quality class label are 5 or 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Preprocessing**: \n",
    "\n",
    "    5.1 For numerical features, determine the best transformation to use. Indicate e transformation that seems appropriate and why. Include the code illustrating how  apply the transformation. For at least one attribute, show the distribution before d after the transformation. See [Preprocessing data](https://scikit-learn.org/able/modules/preprocessing.html).\n",
    "\n",
    "    5.2 For categorical features, show how to apply [one-hot encoding](https://ikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).  your dataset does not have categorical data, show how to apply the one-hot encoder  the label (target variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1a**\n",
    "\n",
    "<u>Preprocessing for features with a normal distribution</u>\n",
    "\n",
    "As we can observe from the histograms in sections 3, we know that only two features have close to a normal distribution: 'density,' and 'pH.' The 'density' feature has a normal distribution without outliers, and the 'pH' feature has a normal distribution with outliers. Thus, since they both have a normal distribution, we want to use standardization, however with the 'pH' feature, since it contains outliers, we want to use robust standardization/the robust scaler since 'scaling using the mean and variance of the data is likely to not work very well [for features with outliers]' (scikit, n.d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "# density feature - normal distribution without outliers\n",
    "scaler = preprocessing.StandardScaler().fit(wq_dataset[['density']])\n",
    "scaler.mean_\n",
    "scaler.scale_\n",
    "density_standardized = scaler.transform(wq_dataset[['density']])\n",
    "# code used from https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "# pH feature - normal distribution with outliers, using robust scaler standardization\n",
    "transformer = RobustScaler().fit(wq_dataset[['pH']])\n",
    "transformer\n",
    "pH_standardized = transformer.transform(wq_dataset[['pH']])\n",
    "# code used from https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2b**\n",
    "\n",
    "<u>Preprocessing for features with a right skewed distribution</u>\n",
    "\n",
    "As we can observe from the histograms in section 3, we know that the remainder of the features do not have a normal distribution, but are skewed to the right/are positively skewed. Thus, we want to use normalization.\n",
    "\n",
    "If we observe the features, some are more right skewed than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 - Applying one-hot encoder to the label/target variable (quality).\n",
    "# My dataset does not have categorical data, so I will show how to apply the one-hot encoder to the quality attribute (target variable)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "res = enc.fit_transform(wq_dataset[['quality']])\n",
    "print(res)\n",
    "# (sckit)\n",
    "# used code from: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Training and target data**: Set the Python variable `X` to designate the data and `y` to designate the target class. Make sure to select only the informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "# the only non-informative feature was already dropped, so all existing features and not the quality feature are informative\n",
    "X = wq_dataset.drop(columns=['quality'])\n",
    "\n",
    "# quality is the target class\n",
    "y = wq_dataset['quality']\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Training and test sets**: Split the dataset into training and testing sets. Reserve 20% of data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# (Turcotte, 2024)\n",
    "# code taken from https://turcotte.xyz/teaching/csi-4106/lectures/05/slides.html#/one-vs-all-classifier-complete\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "\n",
    "# References\n",
    "\n",
    "Hyndman, R. (2010, July 28). *Calculating optimal number of bins in a histogram*. StackExchange. https://stats.stackexchange.com/a/862\n",
    "\n",
    "IBM. (2024, August 14). *What is the K-nearest neighbors algorithm?*. IBM. https://www.ibm.com/topics/knn\n",
    "\n",
    "Liberman, N. (2020, May 21). *Decision trees and random forests*. Medium. https://towardsdatascience.com/decision-trees-and-random-forests-df0c3123f991\n",
    "\n",
    "*Onehotencoder*. scikit. (n.d.). https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html \n",
    "\n",
    "\n",
    "*Preprocessing.* scikit. (n.d.). https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "*RobustScaler.* scikit. (n.d.).https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.RobustScaler.htmlz\n",
    "\n",
    "Turcotte, M. (2024, September 23). *Linear models, logistic regression*. https://turcotte.xyz/teaching/csi-4106/lectures/05/slides.html#/one-vs-all-classifier-complete\n",
    "\n",
    "Turcotte, M. (2024, September 25). *Linear models, training*. https://turcotte.xyz/teaching/csi-4106/lectures/04/slides.html#/\n",
    "\n",
    "W3Schools. (n.d.). *Matplotlib Plotting*. https://www.w3schools.com/python/matplotlib_plotting.asp \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## AI transcript\n",
    "**Hint:** To share a link to your colab notebook, click on \"share\" on the top right. Then, under *General access* , change *Restricted* to \"Anyone with the link\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I used chatGPT-4 and asked it the following question:*\n",
    "\n",
    "If I have features with a normal distribution what should I use, and if I have features that are right/positively skewed, what should I use\n",
    "\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "When dealing with features that exhibit different distributions, the choice between standardization and normalization depends on the characteristics of the data. Here’s how to approach each scenario:\n",
    "\n",
    "Features with a Normal Distribution\n",
    "What to Use: Standardization\n",
    "Reason: Since the data follows a normal distribution, standardization is appropriate. This technique will center the data around a mean of 0 with a standard deviation of 1, making it suitable for algorithms that assume normality (like linear regression, logistic regression, and many machine learning algorithms). Standardizing normally distributed data can help improve convergence during optimization and maintain the relationships between data points.\n",
    "\n",
    "\n",
    "Features that are Right/Positively Skewed\n",
    "What to Use: Normalization (or consider transformation techniques)\n",
    "Reason: For right or positively skewed features, normalization can help rescale the data to a bounded range, making it easier for some algorithms to work with. However, you might also consider transformation techniques such as:\n",
    "Log Transformation: This is effective in reducing skewness by applying the natural logarithm to the data. It can help stabilize variance and make the distribution more normal-like.\n",
    "Square Root Transformation: Similar to log transformation, this can help reduce skewness, especially for count data.\n",
    "Box-Cox Transformation: A family of power transformations that can help normalize data and reduce skewness, applicable when data is strictly positive.\n",
    "Summary\n",
    "For features with a normal distribution, use standardization to scale the data appropriately.\n",
    "For right/positively skewed features, consider normalization or a transformation (like log or square root) to reduce skewness and make the data more suitable for analysis. The choice may depend on the specific characteristics of the data and the algorithms you plan to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
